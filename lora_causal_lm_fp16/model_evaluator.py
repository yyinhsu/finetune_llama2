from reporter import Reporter
from qa_pair_dataset import QAPairDataset
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel, PeftConfig
import torch
from rouge_score import rouge_scorer
import os
import re


class DecorateText:
    def combine_segmented_texts(raw_text: str) -> str:
        """Combine the segmented texts.

        for example:
        raw_text = 'charge: /*ME*/ cause: /*[1. first cause][2. second cause]'*/ \
            action: /*[ME: first action][SW: second action]'*/ \
            prevention: /*[BIOS: first prevention][ME: second prevention]'*/'
        then combine_segmented_texts(raw_text) will return:
        'charge: ME cause: 1. first cause 2. second cause action: ME: first action SW: second action prevention: BIOS: first prevention ME: second prevention'

        :param: str raw_text: the raw text
        :return: the combined text
        :rtype: str
        """
        if '/*' in raw_text: # The segmented texts are involved in /* */
            extracted_text = raw_text.split('/*')[1:]
            extracted_text = [each_text.split('*/')[0] for each_text in extracted_text]
            extracted_text = ' '.join(extracted_text)
            extracted_text = extracted_text.replace('[', '')
            extracted_text = extracted_text.replace(']', '')
            extracted_text = extracted_text.replace('Fixed Action: ', '')
            extracted_text = extracted_text.replace('Prevention: ', '')
            extracted_text = extracted_text.replace('charge: ', '')
            extracted_text = extracted_text.replace('cause: ', '')
            extracted_text = extracted_text.replace('action: ', '')
            extracted_text = extracted_text.replace('prevention: ', '')
        elif '-*' in raw_text: # The segmented texts are listed with -*
            extracted_text = raw_text.replace('responsibility: ', '')
            extracted_text = extracted_text.replace('charge: ', '')
            extracted_text = extracted_text.replace('cause: ', '')
            extracted_text = extracted_text.replace('action: ', '')
            extracted_text = extracted_text.replace('prevention: ', '')
            extracted_text = extracted_text.replace('-*', '')
        elif '/*[' in raw_text:
            pass ##TODO: add code here
        else:
            extracted_text = raw_text

        return extracted_text


    def fill_segmented_text_in_preview_answer_template(raw_text: str) -> str:
        """Fill the texts generated by model in the preview answer template.

        :param: str raw_text: the raw text
        :return: the filled text
        :rtype: str
        """
        answer_template = "The /*{func_text}*/ function team is in charge of the issue. The issue is caused by: /*{causes_text}*/ The fixed actions of function teams are as follows: /*{actions_text}*/ The prevention measures of function teams are as follows: /*{preventions_text}*/"
        if '/*' in raw_text: # The segmented texts are involved in /* */
            # extracted_text = raw_text.split('/*')[1:]
            # extracted_text = [each_text.split('*/')[0] for each_text in extracted_text]
            unknown_text = ' unknown '
            extracted_func = re.findall(r"charge:\s*/\*(.*?)\*/", raw_text)
            extracted_func = extracted_func[0] if extracted_func else unknown_text
            extracted_causes = re.findall(r"cause:\s*/\*(.*?)\*/", raw_text)
            extracted_causes = extracted_causes[0] if extracted_causes else unknown_text
            extracted_actions = re.findall(r"action:\s*/\*(.*?)\*/", raw_text)
            extracted_actions = extracted_actions[0] if extracted_actions else unknown_text
            extracted_preventions = re.findall(r"prevention:\s*/\*(.*?)\*/", raw_text)
            extracted_preventions = extracted_preventions[0] if extracted_preventions else unknown_text

            preview_ans = answer_template.replace('{func_text}', extracted_func)
            preview_ans = preview_ans.replace('{causes_text}', extracted_causes)
            preview_ans = preview_ans.replace('{actions_text}', extracted_actions)
            preview_ans = preview_ans.replace('{preventions_text}', extracted_preventions)

            preview_ans = preview_ans.replace(' , ', ', ').replace(' ,', ',').replace(' .', '.')
        elif '-*' in raw_text: # The segmented texts are listed with -*
            preview_ans = ''
        elif '/*[' in raw_text:
            pass ##TODO: add code here
        else:
            preview_ans = raw_text

        return preview_ans




class MetricMaker():
    """This class is used to calculate the metric ROUGE score of the model performance."""
    def get_avg_rouge_score(preds: list, ground_truths: list) -> dict:
        """Calculate the average ROUGE score of the model performance.

        :param: list preds: the list of the predictions
        :param: list ground_truths: the list of the ground truths
        :return: the average ROUGE score invloving rouge1, rouge2, rougeL, rougeLsum of the model performance
        :rtype: dict
        """
        ## Set up rouge scorer
        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL', 'rougeLsum'], use_stemmer=True)
        ## Calculate rouge score and store the scores in score_sum_dict
        score_sum_dict = {'rouge1': 0, 'rouge2': 0, 'rougeL': 0, 'rougeLsum': 0}
        for pred, truth in zip(preds, ground_truths):
            scores = scorer.score(pred, truth)
            for key in score_sum_dict.keys():
                score_sum_dict[key] += scores[key].fmeasure
        ## Calculate average scores
        for key in score_sum_dict.keys():
            score_sum_dict[key] /= len(preds)

        return score_sum_dict


    def get_rouge_score(prediction: str, ground_truth: str) -> dict:
        """Calculate the ROUGE score of prediction text and ground truth text.

        :param: str prediction: the prediction text
        :param: str ground_truth: the ground truth text
        :return: the ROUGE score invloving rouge1, rouge2, rougeL, rougeLsum of the model performance
        :rtype: dict
        """
        ## Set up rouge scorer
        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL', 'rougeLsum'], use_stemmer=True)
        ## Calculate rouge score of predicted text and ground truth text, and store the scores in score_dict
        score = scorer.score(prediction, ground_truth)
        score_dict = {'rouge1': score['rouge1'].fmeasure, 'rouge2': score['rouge2'].fmeasure,
                      'rougeL': score['rougeL'].fmeasure, 'rougeLsum': score['rougeLsum'].fmeasure}

        return score_dict


    def get_avg_segmented_rouge_score(preds: list, ground_truths: list) -> dict:
        """Calculate the average ROUGE score of the segmented texts (who, causes, opportunities (action & prevention)) of the model performance.

        :param: list preds: the list of the predictions
        :param: list ground_truths: the list of the ground truths
        :return: the average ROUGE score invloving rouge1, rouge2, rougeL, rougeLsum of the model performance
        :rtype: dict
        """
        ## Set up rouge scorer
        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL', 'rougeLsum'], use_stemmer=True)
        score_sum_dict = {'rouge1': 0, 'rouge2': 0, 'rougeL': 0, 'rougeLsum': 0}

        ## get function team, causes, and oppportunities from the prediction and ground truth, they are involved in /* */
        ## then combine function team, causes, and oppportunities to form a new prediction and ground truth
        for pred, truth in zip(preds, ground_truths):
            pred = DecorateText.combine_segmented_texts(pred)
            truth = DecorateText.combine_segmented_texts(truth)
            scores = scorer.score(pred, truth)
            for key in score_sum_dict.keys():
                score_sum_dict[key] += scores[key].fmeasure
        ## Calculate average scores
        for key in score_sum_dict.keys():
            score_sum_dict[key] /= len(preds)

        return score_sum_dict


    def get_segmented_rouge_score(prediction: str, ground_truth: str):
        """Calculate the ROUGE score of the segmented texts (who, causes, opportunities (action & prevention)) of the model performance.

        :param: str prediction: the prediction text
        :param: str ground_truth: the ground truth text
        :return: the ROUGE score invloving rouge1, rouge2, rougeL, rougeLsum of the model performance
        :rtype: dict
        """
        ## Set up rouge scorer
        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL', 'rougeLsum'], use_stemmer=True)
        ## Calculate rouge score of predicted text and ground truth text, and store the scores in score_dict
        prediction = DecorateText.combine_segmented_texts(prediction)
        ground_truth = DecorateText.combine_segmented_texts(ground_truth)
        score = scorer.score(prediction, ground_truth)
        score_dict = {'rouge1': score['rouge1'].fmeasure, 'rouge2': score['rouge2'].fmeasure,
                      'rougeL': score['rougeL'].fmeasure, 'rougeLsum': score['rougeLsum'].fmeasure}

        return score_dict, ground_truth, prediction ##TODO: remove prediction from here, and add "-> dict" to the function signature


    def get_preview_rouge_score(prediction: str, ground_truth: str) -> dict:
        """Calculate the ROUGE score of the prediction text and ground truth text, both of which are filled in the preview answer template.

        :param: str prediction: the prediction text
        :param: str ground_truth: the ground truth text
        :return: the ROUGE score invloving rouge1, rouge2, rougeL, rougeLsum of the model performance
        :rtype: dict
        """
        ## Set up rouge scorer
        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL', 'rougeLsum'], use_stemmer=True)
        ## Calculate rouge score of predicted text and ground truth text, and store the scores in score_dict
        prediction = DecorateText.fill_segmented_text_in_preview_answer_template(prediction)
        ground_truth = DecorateText.fill_segmented_text_in_preview_answer_template(ground_truth)
        score = scorer.score(prediction, ground_truth)
        score_dict = {'rouge1': score['rouge1'].fmeasure, 'rouge2': score['rouge2'].fmeasure,
                      'rougeL': score['rougeL'].fmeasure, 'rougeLsum': score['rougeLsum'].fmeasure}

        return score_dict, ground_truth, prediction




class ModelEvaluator():
    """Evaluate the model on the evaluation dataset, and present the evaluation metrics."""
    def __init__(self, peft_model_folder: str, base_model: str, reporter: Reporter) -> None:
        """Initialize the model evaluator.

        :param: str peft_model_folder: the folder name of the prefix tuning model
        :param: str base_model: the base model of the fine-tuned model
        :param: Reporter reporter: the reporter to record the model performance
        """
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model = self.load_trained_model(peft_model_folder, base_model)
        self.base_model = base_model
        self.reporter = reporter


    def evaluate(self, eval_dataloader: torch.utils.data.DataLoader, tokenizer: AutoTokenizer) -> dict:
        """Evaluate the model on the evaluation dataset.

        print the accuracy and the first 10 predictions and true labels.

        :param: torch.utils.data.DataLoader eval_dataloader: the evaluation dataloader
        :param: AutoTokenizer tokenizer: the tokenizer
        :return: rouge_score: the ROUGE score of the model on the evaluation dataset
        :rtype: rouge_score: dict
        """
        ## Get the prediction texts and true label texts of the evaluation dataset
        predictions = self.__inference_eval_dataloader(eval_dataloader, tokenizer)
        ground_truths = self.__decode_with_dataloader(eval_dataloader, tokenizer)
        rouge_score = MetricMaker.get_avg_rouge_score(predictions, ground_truths)
        ## Write the evaluation result to the training_report.txt
        self.reporter.write_evaluating_result(rouge_score)
        # self.reporter.write_log(f"prediction: {predictions[:10]}")
        # self.reporter.write_log(f"ground_truth: {ground_truths[:10]}")
        ## Print the evaluation result
        print(f"{rouge_score=}")
        print(f"{predictions[:10]=}")
        print(f"{ground_truths[:10]=}")

        return rouge_score


    def load_trained_model(self, peft_model_folder: str, base_model: str) -> PeftModel:
        """Load the model and add the prefix tuning configuration to the model.

        :param str. peft_model_folder: the folder name of the prefix tuning model
        :return model: the model with the prefix tuning configuration
        :rtype model: PeftModel
        """
        model_path = os.path.join(peft_model_folder, 'peft_model')

        nf4_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            # bnb_4bit_use_double_quant=True,
            bnb_4bit_compute_dtype=torch.bfloat16
            )
        print("base_model: ", base_model)
        model = AutoModelForCausalLM.from_pretrained(base_model, quantization_config=nf4_config, load_in_4bit=True, device_map="auto")
        model = PeftModel.from_pretrained(model, model_id=model_path) # model_id: A path to a directory containing a Lora configuration file saved

        return model


    def produce_inference_report(self,
                                 train_params: dict,
                                 dataset_name: str,
                                 dataset: QAPairDataset,
                                 tokenizer: AutoTokenizer,
                                 report_file_prefix: str) -> None:

        """Produce the inference report of the model on the evaluation dataset.

        This report contains the training parameters, the average ROUGE score of the model on the evaluation dataset,
        and predictions and true labels of data.
        This report is saved as a json file.

        :param: dict train_params: the training parameters
        :param: str dataset_name: the registered name of the dataset on azure ml workspace
        :param: QAPairsDataset dataset: the evaluation dataset
        :param: AutoTokenizer tokenizer: the tokenizer
        :param: str report_file_prefix: the prefix of the report file name
        """
        report_json_dict = {}
        report_json_dict['train_params'] = train_params
        report_json_dict['dataset_name'] = dataset_name
        report_json_dict['avg_rouge_score'] = {}
        report_json_dict['avg_preview_rouge_score'] = {}
        report_json_dict['avg_segmented_rouge_score'] = {}
        report_json_dict['max_rouge_score'] = {}
        report_json_dict['max_preview_rouge_score'] = {}
        report_json_dict['max_segmented_rouge_score'] = {}
        report_json_dict['min_rouge_score'] = {}
        report_json_dict['min_preview_rouge_score'] = {}
        report_json_dict['min_segmented_rouge_score'] = {}
        report_json_dict['data'] = []

        text_data_dicts = dataset.data_dicts
        inputs = [data['input'] for data in text_data_dicts]
        ground_truths = [data['output'] for data in text_data_dicts]
        input_max_length = dataset.a_max_length
        output_max_length = dataset.a_max_length

        predictions = self.__inference_eval_data_text(input_text_list=inputs,
                                                      tokenizer=tokenizer,
                                                      input_max_length=input_max_length,
                                                      output_max_length=output_max_length)

        rouge_scores_dicts = []

        for enum, (input, pred, truth) in enumerate(zip(inputs, predictions, ground_truths)):
            score_dict = MetricMaker.get_rouge_score(pred, truth)

            seg_truth = DecorateText.combine_segmented_texts(truth)
            seg_pred = DecorateText.combine_segmented_texts(pred)
            segmented_rouge_score = MetricMaker.get_rouge_score(seg_pred, seg_truth)

            preview_truth = DecorateText.fill_segmented_text_in_preview_answer_template(truth)
            preview_pred = DecorateText.fill_segmented_text_in_preview_answer_template(pred)
            preview_rouge_score = MetricMaker.get_rouge_score(preview_pred, preview_truth)

            rouge_scores_dicts.append({'raw': score_dict,
                                      'preview': preview_rouge_score,
                                      'segmented': segmented_rouge_score})

            data_dict = {'sequential_id': enum,
                         'input': input,
                         'ground_truth': truth,
                         'prediction': pred,
                         'preview_ground_truth': preview_truth,
                         'preview_prediction': preview_pred,
                         'segemented_ground_truths': seg_truth, ##TODO: remove seg_truths from here
                         'segmented_preds': seg_pred, ##TODO: remove seg_texts from here
                         'rouge_score': score_dict,
                         'preview_rouge_score': preview_rouge_score,
                         'segmented_rouge_score': segmented_rouge_score,
                         }
            report_json_dict['data'].append(data_dict)

        ## Initialize summary_rouge_dict:
        ## 1st level of keys: initialize in format_summary_keys
        ## 2nd level of keys: 'avg', 'max', 'min'
        ## 3rd level of keys: see avg_dict, max_dict, and min_dict
        ## for example: summary_rouge_key['segmented']['min']['min_rouge1_sequential_id']
        format_summary_keys = ['raw', 'preview', 'segmented']
        statistic_keys = ['avg', 'max', 'min']
        rouge_keys = ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']

        ## Initialize summary_rouge_dict
        summary_rouge_dict = {f_key: {stat_key: {} for stat_key in statistic_keys} for f_key in format_summary_keys}
        for f_key in format_summary_keys:
            for r_key in rouge_keys:
                summary_rouge_dict[f_key]['avg'][r_key] = 0
                summary_rouge_dict[f_key]['max'][r_key] = 0
                summary_rouge_dict[f_key]['max'][f'max_{r_key}_sequential_id'] = ""
                summary_rouge_dict[f_key]['min'][r_key] = 1
                summary_rouge_dict[f_key]['min'][f'min_{r_key}_sequential_id'] = ""

        ## Calculate average, max, min of summary_rouge_dict
        for seq_id, score_dict in enumerate(rouge_scores_dicts):
            for f_key in format_summary_keys:
                for r_key in rouge_keys:
                    summary_rouge_dict[f_key]['avg'][r_key] += score_dict[f_key][r_key]

                    if score_dict[f_key][r_key] >= summary_rouge_dict[f_key]['max'][r_key]:
                        summary_rouge_dict[f_key]['max'][r_key] = score_dict[f_key][r_key]
                        if score_dict[f_key][r_key] == summary_rouge_dict[f_key]['max'][r_key]:
                            summary_rouge_dict[f_key]['max'][f'max_{r_key}_sequential_id'] += f'{seq_id}, '
                        elif score_dict[f_key][r_key] > summary_rouge_dict[f_key]['max'][r_key]:
                            summary_rouge_dict[f_key]['max'][f'max_{r_key}_sequential_id'] = f'{seq_id}'

                    if score_dict[f_key][r_key] <= summary_rouge_dict[f_key]['min'][r_key]:
                        summary_rouge_dict[f_key]['min'][r_key] = score_dict[f_key][r_key]
                        if score_dict[f_key][r_key] == summary_rouge_dict[f_key]['min'][r_key]:
                            summary_rouge_dict[f_key]['min'][f'min_{r_key}_sequential_id'] += f'{seq_id}, '
                        elif score_dict[f_key][r_key] < summary_rouge_dict[f_key]['min'][r_key]:
                            summary_rouge_dict[f_key]['min'][f'min_{r_key}_sequential_id'] = f'{seq_id}'

        for f_key in format_summary_keys:
            for r_key in rouge_keys:
                ## Calculate average
                summary_rouge_dict[f_key]['avg'][r_key] = summary_rouge_dict[f_key]['avg'][r_key] / len(rouge_scores_dicts)
                ## Remove the last ', ' in max and min sequential_id
                summary_rouge_dict[f_key]['max'][f'max_{r_key}_sequential_id'] = summary_rouge_dict[f_key]['max'][f'max_{r_key}_sequential_id'][:-2]
                summary_rouge_dict[f_key]['min'][f'min_{r_key}_sequential_id'] = summary_rouge_dict[f_key]['min'][f'min_{r_key}_sequential_id'][:-2]

        ## Add summary_rouge_dict to report_json_dict
        report_json_dict['avg_rouge_score'] = summary_rouge_dict['raw']['avg']
        report_json_dict['avg_segmented_rouge_score'] = summary_rouge_dict['segmented']['avg']
        report_json_dict['avg_preview_rouge_score'] = summary_rouge_dict['preview']['avg']

        report_json_dict['max_rouge_score'] = summary_rouge_dict['raw']['max']
        report_json_dict['max_segmented_rouge_score'] = summary_rouge_dict['segmented']['max']
        report_json_dict['max_preview_rouge_score'] = summary_rouge_dict['preview']['max']

        report_json_dict['min_rouge_score'] = summary_rouge_dict['raw']['min']
        report_json_dict['min_segmented_rouge_score'] = summary_rouge_dict['segmented']['min']
        report_json_dict['min_preview_rouge_score'] = summary_rouge_dict['preview']['min']

        self.reporter.write_inference_report_to_json(report_json_dict, report_file_prefix)


    def __decode_with_dataloader(self, dataloader: torch.utils.data.DataLoader, tokenizer: AutoTokenizer) -> list:
        """Decode the labels which is the ground truth of the data in the dataloader.

        The labels are decoded by the tokenizer.
        The reason why we need to decode the labels is that we have to compute metric ROUGE score.
        If we use the original text from the dataset, the special tokens and unrefined syntax would affect the ROUGE score.

        :param: torch.utils.data.DataLoader dataloader: the dataloader of the training or evaluating dataset
        :param: AutoTokenizer tokenizer: the tokenizer
        :return: the decoded texts of the labels
        :rtype: list
        """
        decoded_texts = []
        for _, batch in enumerate(dataloader):
            if 1 in batch['labels'].shape: # batch_size = 1 or the last batch
                to_be_decoded = [token for token in batch['labels'].squeeze().detach().tolist() if token != -100]
                decoded = tokenizer.decode(to_be_decoded, skip_special_tokens=True)
                decoded_texts.append(decoded)
            else:
                for pred in batch['labels'].squeeze().detach().tolist():
                    to_be_decoded = [token for token in pred if token != -100]
                    decoded = tokenizer.decode(to_be_decoded, skip_special_tokens=True)
                    decoded_texts.append(decoded)

        return decoded_texts


    def __inference_eval_data_text(self, input_text_list: list, tokenizer: AutoTokenizer, input_max_length: int, output_max_length: int) -> list:
        """Inference the model on the evaluation dataset.

        Generation config is set to max_new_tokens, repetition_penalty=1.2.
        ref: https://huggingface.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationConfig

        :param: list. input_text_list: the list of the input text
        :param: AutoTokenizer tokenizer: the tokenizer
        :param: int input_max_length: the maximum length of the input text
        :param: int output_max_length: the maximum length of the output text
        :return: the decoded predictions of the evaluation dataset
        :rtype: list
        """
        self.model.to(self.device)

        predictions = []
        for text in input_text_list:
            tokenized_inputs = tokenizer(text + "<answer>",
                                         return_tensors="pt")

            tokenized_inputs['input_ids'] = tokenized_inputs['input_ids'].to(self.device)
            tokenized_inputs['attention_mask'] = tokenized_inputs['attention_mask'].to(self.device)
            with torch.no_grad():
                outputs = self.model.generate(input_ids=tokenized_inputs['input_ids'],
                                            #   attention_mask=tokenized_inputs['attention_mask'],
                                              max_new_tokens=input_max_length + output_max_length)
            predictions.append(outputs.detach().cpu().numpy().tolist()[0])

        decoded_predictions = []
        for pred in predictions:
            to_be_decoded = [token for token in pred if token != -100]
            decoded = tokenizer.decode(to_be_decoded, skip_special_tokens=True)
            try:
                pattern = "\s<answer>\s(.*?)\s</answer>"
                decoded = re.findall(pattern, decoded)[0]
            except:
                pass
            decoded_predictions.append(decoded)

        return decoded_predictions


    def __inference_eval_dataloader(self, eval_dataloader: torch.utils.data.DataLoader, tokenizer: AutoTokenizer) -> list:
        """Inference the model on the evaluation dataset.

        Generation config is set to max_new_tokens, repetition_penalty=1.2.
        ref: https://huggingface.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationConfig

        :param: torch.utils.data.DataLoader eval_dataloader: the evaluation dataloader
        :param: AutoTokenizer tokenizer: the tokenizer
        :return: the decoded predictions of the evaluation dataset
        :rtype: list
        """
        self.model.to(self.device)

        predictions = []
        for _, batch in enumerate(eval_dataloader): #TODO: one by one, need padding and truncate
            batch['input_ids'] = batch['input_ids'].to(self.device)
            batch['attention_mask'] = batch['attention_mask'].to(self.device)
            # max_new_tokens = batch['labels'].detach().cpu().numpy().shape[-1]
            max_new_tokens = eval_dataloader.dataset.a_max_length
            with torch.no_grad():
                outputs = self.model.generate(input_ids=batch['input_ids'],
                                            #   attention_mask=batch['attention_mask'],
                                              max_new_tokens=max_new_tokens,
                                            #   pad_token_id=tokenizer.eos_token_id
                                              )
            predictions.extend(outputs.detach().cpu().numpy().tolist())

        decoded_predictions = []
        for pred in predictions:
            to_be_decoded = [token for token in pred if token != -100]
            decoded = tokenizer.decode(to_be_decoded, skip_special_tokens=True)
            decoded_predictions.append(decoded)

        return decoded_predictions


if __name__ == '__main__':
    pass