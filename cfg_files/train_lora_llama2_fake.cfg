[DATASET]
dataset_version: v1
train_dataset_path: data/data_${DATASET:dataset_version}/training_data.jsonl
test_dataset_path: data/data_${DATASET:dataset_version}/testing_data.jsonl

[PRETRAINED_MODEL]
pretrained_model: meta-llama/Llama-2-7b-chat-hf
huggingface_hub_token: hf_***************************
input_max_length: -1 # to be determined in prepare_data.py
output_max_length: -1 # to be determined in prepare_data.py

[TRAINING]
note: first_training
finetuning_algo: lora
finetuned_model_prefix: model_v1
batch_size: 2
num_epochs: 2
learning_rate: 0.001
weight_decay: 0.01
lora_rank: 16
lora_alpha: 16
lora_dropout: 0.2

[INFERENCE]
input_path_1: data/data_${DATASET:dataset_version}/testing_data.jsonl
input_path_2: data/data_${DATASET:dataset_version}/validating_data.jsonl
inference_model: ${TRAINING:model_name}
output_path_1: inference_${TRAINING:model_name}_testing_set.csv
output_path_2: inference_${TRAINING:model_name}_validating_set.csv
